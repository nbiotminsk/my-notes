# Как подключить свою AI-модель в Gemini CLI (OpenAI-совместимую)

Это руководство объясняет, как подключить и использовать вашу собственную AI-модель, совместимую с API OpenAI, в Gemini CLI. Это может быть локальная модель (например, через Ollama) или любой другой облачный сервис.

На данный момент Gemini CLI **не поддерживает** прямое добавление кастомных моделей через `settings.json`. Вместо этого используется специальный **сервер-мост (bridge)**, который выступает в роли "переводчика" между Gemini и вашей моделью.

---

### Концепция: Как это работает?

1.  **Запуск сервера-моста:** Вы запускаете на своем компьютере специальный локальный сервер.
2.  **Передача данных:** Этому серверу вы сообщаете адрес (`base_url`) и ключ (`api_key`) вашей кастомной модели.
3.  **Подключение в Gemini:** В настройках Gemini CLI вы подключаетесь к этому локальному серверу как к MCP-инструменту.
4.  **Перенаправление запроса:** Когда вы в Gemini обращаетесь к вашей модели, запрос идет на сервер-мост, а тот уже перенаправляет его на `base_url` вашей модели с вашим ключом.

---

### Шаг 1: Установка сервера-моста

Мы будем использовать пакет `@intelligentinternet/gemini-cli-mcp-openai-bridge`. Установите его глобально с помощью npm.

Вы можете попросить Gemini сделать это за вас:

> !npm install -g @intelligentinternet/gemini-cli-mcp-openai-bridge

### Шаг 2: Запуск сервера-моста

Теперь вам нужно запустить этот сервер в **отдельном окне терминала**. При запуске вы должны указать ему адрес и ключ вашей модели.

Откройте новый терминал и выполните команду:

```
gemini-cli-bridge
```

```bash
gemini-cli-mcp-openai-bridge --port 8080 --base-url <ВАШ_BASE_URL> --api-key <ВАШ_API_KEY>
```

*   `--port 8080`: Указывает, что сервер будет работать на порту 8080. Можете выбрать другой, если этот занят.
*   `--base-url`: **Обязательно** замените `<ВАШ_BASE_URL>` на адрес вашей модели (например, `http://localhost:11434/v1` для Ollama).
*   `--api-key`: **Обязательно** замените `<ВАШ_API_KEY>` на ваш ключ. Если ключ не требуется, укажите `none`.

**Важно:** Не закрывайте это окно терминала, пока вы хотите использовать вашу модель. Сервер-мост должен быть постоянно запущен.

### Шаг 3: Настройка Gemini CLI

Теперь, когда мост запущен, нужно "сообщить" о нем Gemini CLI. Для этого отредактируйте файл `settings.json`.

1.  **Найдите `settings.json`:** Он может быть в папке `.gemini/` вашего проекта или в домашней директории `~/.gemini/`.
2.  **Добавьте конфигурацию:** Добавьте в `mcpServers` новый сервер. Поскольку вы уже запустили его вручную, достаточно указать его адрес:

```json
{
    "mcpServers": {
        "my-model": {
            "address": "http://localhost:8080"
        }
    }
}
```

*   `"my-model"`: Это имя, по которому вы будете обращаться к вашей модели в Gemini. Вы можете изменить его на любое другое (например, `"ollama"`, `"local-llama"`).

### Шаг 4: Использование вашей модели

Перезапустите Gemini CLI, если он был открыт. Теперь вы можете обращаться к вашей модели по имени, которое вы задали в `settings.json`.

**Пример запроса:**

> @my-model какая у тебя версия?

Gemini отправит этот запрос на ваш локальный мост (порт 8080), а тот перенаправит его на вашу кастомную модель.

---

#gemini #cli #tutorial #ai #custom-model #openai #mcp
